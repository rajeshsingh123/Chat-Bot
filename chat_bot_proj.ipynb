{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk+EquFayOPGC7H2Mm9JRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsingh123/Chat-Bot/blob/main/chat_bot_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BvDJ5tVcaBn",
        "outputId": "db9f79d2-eb13-4d2e-db93-877922a46aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow==1.13.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRTGsQn0ctg_",
        "outputId": "cf68b007-2626-4960-9801-3c158d71a19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 27 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.47.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.2.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.21.6)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.5.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp_uXa5XcviW",
        "outputId": "67f6f87e-5f60-456b-cb30-59b40d24a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tflearn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1lsE2lmdE1d",
        "outputId": "830b8289-4f0b-4cf9-8cd2-70d572e7e9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=d176a1496734baecd465209ec68c61d450a40791be3bb67a2ff98949af46d0eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries required for NLP\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Importing Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf   #version 1.13.2\n",
        "import numpy as np\n",
        "import tflearn            #version 0.3.2\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5kDzMjBc6M5",
        "outputId": "049442a3-6051-48af-c216-97c30c0f7c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing our intent file used for training the model.\n",
        "with open(\"intents.json\") as json_data: \n",
        "    intents = json.load(json_data)      # Loading data from intents.json file to var intents"
      ],
      "metadata": {
        "id": "t8p0bv-4c90j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty lists for appending the data after processing NLP\n",
        "words=[]\n",
        "documents = []\n",
        "classes = []\n",
        "\n",
        "\n",
        "# This list will be used for ignoring all unwanted punctuation marks.\n",
        "ignore = [\"?\"]\n",
        "\n",
        "# Starting a loop through each intent in intents[\"patterns\"]\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        \n",
        "        # tokenizing each and every word in the sentence by using word tokenizer and storing in w\n",
        "        w = nltk.word_tokenize(pattern) \n",
        "        #print(w)\n",
        "        \n",
        "        # Adding tokenized words to words empty list that we created\n",
        "        words.extend(w) \n",
        "        #print(words)\n",
        "        \n",
        "        # Adding words to documents with tag given in intents file\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        #print(documents)\n",
        "        \n",
        "        # Adding only tag to our classes list\n",
        "        if intent[\"tag\"] not in classes:      \n",
        "            classes.append(intent[\"tag\"])  #If tag is not present in classes[] then it will append into it.\n",
        "            #print(classes)"
      ],
      "metadata": {
        "id": "T50SIe4gdK1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqAXtyrUdR-r",
        "outputId": "3eb0a7fc-621a-44db-af7a-7191ddc79c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " 'Is',\n",
              " 'anyone',\n",
              " 'there',\n",
              " '?',\n",
              " 'Hello',\n",
              " 'Good',\n",
              " 'day',\n",
              " 'Bye',\n",
              " 'See',\n",
              " 'you',\n",
              " 'later',\n",
              " 'Goodbye',\n",
              " 'Thanks',\n",
              " 'Thank',\n",
              " 'you',\n",
              " 'That',\n",
              " \"'s\",\n",
              " 'helpful',\n",
              " 'What',\n",
              " 'hours',\n",
              " 'are',\n",
              " 'you',\n",
              " 'open',\n",
              " '?',\n",
              " 'What',\n",
              " 'are',\n",
              " 'your',\n",
              " 'hours',\n",
              " '?',\n",
              " 'When',\n",
              " 'are',\n",
              " 'you',\n",
              " 'open',\n",
              " '?',\n",
              " 'When',\n",
              " 'is',\n",
              " 'the',\n",
              " 'time',\n",
              " 'to',\n",
              " 'contact',\n",
              " '?',\n",
              " 'At',\n",
              " 'what',\n",
              " 'time',\n",
              " 'do',\n",
              " 'you',\n",
              " 'provide',\n",
              " 'services',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'your',\n",
              " 'location',\n",
              " '?',\n",
              " 'Where',\n",
              " 'are',\n",
              " 'you',\n",
              " 'located',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'your',\n",
              " 'address',\n",
              " '?',\n",
              " 'Where',\n",
              " 'is',\n",
              " 'your',\n",
              " 'company',\n",
              " 'situated',\n",
              " '?',\n",
              " 'How',\n",
              " 'can',\n",
              " 'we',\n",
              " 'contact',\n",
              " 'you',\n",
              " '?',\n",
              " 'How',\n",
              " 'can',\n",
              " 'I',\n",
              " 'contact',\n",
              " 'you',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'take',\n",
              " 'credit',\n",
              " 'cards',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'accept',\n",
              " 'Mastercard',\n",
              " '?',\n",
              " 'Are',\n",
              " 'you',\n",
              " 'cash',\n",
              " 'only',\n",
              " '?',\n",
              " 'Which',\n",
              " 'credit',\n",
              " 'cards',\n",
              " 'do',\n",
              " 'you',\n",
              " 'accept',\n",
              " '?',\n",
              " 'What',\n",
              " 'are',\n",
              " 'the',\n",
              " 'services',\n",
              " 'that',\n",
              " 'you',\n",
              " 'provide',\n",
              " '?',\n",
              " 'Which',\n",
              " 'services',\n",
              " 'do',\n",
              " 'you',\n",
              " 'provide',\n",
              " '?',\n",
              " 'How',\n",
              " 'can',\n",
              " 'you',\n",
              " 'help',\n",
              " 'us',\n",
              " '?',\n",
              " 'What',\n",
              " 'can',\n",
              " 'you',\n",
              " 'do',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'provide',\n",
              " 'industrial',\n",
              " 'training',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'deliver',\n",
              " 'webinars',\n",
              " 'on',\n",
              " 'cybersecurity',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'provide',\n",
              " 'Summer',\n",
              " 'training',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'conduct',\n",
              " 'workshops',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'mean',\n",
              " 'by',\n",
              " 'cryptography',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Cryptography',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'a',\n",
              " 'Firewall',\n",
              " 'and',\n",
              " 'why',\n",
              " 'is',\n",
              " 'it',\n",
              " 'used',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'firewall',\n",
              " '?',\n",
              " 'what',\n",
              " 'firewall',\n",
              " 'does',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Penetration',\n",
              " 'Testing',\n",
              " '?',\n",
              " 'explain',\n",
              " 'Penetration',\n",
              " 'Testing',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'in',\n",
              " 'cyber',\n",
              " 'security',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Vulnerability',\n",
              " 'Assessment',\n",
              " '?',\n",
              " 'explain',\n",
              " 'Vulnerability',\n",
              " 'Assessment',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'VA',\n",
              " 'in',\n",
              " 'cyber',\n",
              " 'security',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'traceroute',\n",
              " '?',\n",
              " 'explain',\n",
              " 'traceroute',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'a',\n",
              " 'VPN',\n",
              " '?',\n",
              " 'explain',\n",
              " 'vpn',\n",
              " '?',\n",
              " 'why',\n",
              " 'vpn',\n",
              " 'is',\n",
              " 'used',\n",
              " '?',\n",
              " 'what',\n",
              " 'are',\n",
              " 'black',\n",
              " 'hat',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'black',\n",
              " 'hat',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'what',\n",
              " 'are',\n",
              " 'White',\n",
              " 'hat',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'ethical',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'what',\n",
              " 'are',\n",
              " 'Grey',\n",
              " 'hat',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'Grey',\n",
              " 'hat',\n",
              " 'hackers',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'an',\n",
              " 'ARP',\n",
              " 'and',\n",
              " 'how',\n",
              " 'does',\n",
              " 'it',\n",
              " 'work',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'ARP',\n",
              " '?',\n",
              " 'arp',\n",
              " 'in',\n",
              " 'cyber',\n",
              " 'security',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'a',\n",
              " 'Botnet',\n",
              " '?',\n",
              " 'Bonet',\n",
              " '?',\n",
              " 'ssl',\n",
              " '?',\n",
              " 'explain',\n",
              " 'ssl',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'ssl',\n",
              " '?',\n",
              " 'tls',\n",
              " '?',\n",
              " 'explain',\n",
              " 'tls',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'tls',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Cognitive',\n",
              " 'Cybersecurity',\n",
              " '?',\n",
              " 'explain',\n",
              " 'Cognitive',\n",
              " 'Cybersecurity',\n",
              " '?',\n",
              " 'Cognitive',\n",
              " 'Cybersecurity',\n",
              " '?',\n",
              " 'How',\n",
              " 'is',\n",
              " 'Encryption',\n",
              " 'different',\n",
              " 'from',\n",
              " 'Hashing',\n",
              " '?',\n",
              " 'Encryption',\n",
              " 'different',\n",
              " 'from',\n",
              " 'Hashing',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'encryption',\n",
              " '?',\n",
              " 'encryption',\n",
              " '?',\n",
              " 'explain',\n",
              " 'encryption',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Decryption',\n",
              " '?',\n",
              " 'Decryption',\n",
              " '?',\n",
              " 'explain',\n",
              " 'Decryption',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Hashing',\n",
              " '?',\n",
              " 'Hashing',\n",
              " '?',\n",
              " 'explain',\n",
              " 'Hashing',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'cybersecurity',\n",
              " '?',\n",
              " 'cybersecurity',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'forward',\n",
              " 'secrecy',\n",
              " '?',\n",
              " 'Forward',\n",
              " 'Secrecy',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'boot',\n",
              " 'sector',\n",
              " 'virus',\n",
              " '?',\n",
              " 'how',\n",
              " 'boot',\n",
              " 'sector',\n",
              " 'virus',\n",
              " 'effects',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'direct',\n",
              " 'action',\n",
              " 'virus',\n",
              " '?',\n",
              " 'how',\n",
              " 'direct',\n",
              " 'action',\n",
              " 'virus',\n",
              " 'effects',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'resident',\n",
              " 'virus',\n",
              " '?',\n",
              " 'how',\n",
              " 'resident',\n",
              " 'virus',\n",
              " 'effects',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'multipartite',\n",
              " 'virus',\n",
              " '?',\n",
              " 'how',\n",
              " 'multipartite',\n",
              " 'virus',\n",
              " 'effects',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'spacefiller',\n",
              " 'virus',\n",
              " '?',\n",
              " 'how',\n",
              " 'spacefiller',\n",
              " 'virus',\n",
              " 'effects',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'provide',\n",
              " 'home',\n",
              " 'delivery',\n",
              " '?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'deliver',\n",
              " 'the',\n",
              " 'food',\n",
              " '?',\n",
              " 'What',\n",
              " 'are',\n",
              " 'the',\n",
              " 'home',\n",
              " 'delivery',\n",
              " 'options',\n",
              " '?',\n",
              " 'what',\n",
              " 'is',\n",
              " 'computer',\n",
              " 'virus',\n",
              " '?',\n",
              " 'What',\n",
              " 'do',\n",
              " 'you',\n",
              " 'mean',\n",
              " 'by',\n",
              " 'computer',\n",
              " 'virus',\n",
              " '?',\n",
              " 'Define',\n",
              " 'Computer',\n",
              " 'Virus',\n",
              " 'Important',\n",
              " 'Features',\n",
              " 'of',\n",
              " 'a',\n",
              " 'Cybersecurity',\n",
              " '?',\n",
              " 'Why',\n",
              " 'we',\n",
              " 'need',\n",
              " 'CyberSecurity',\n",
              " '?',\n",
              " 'Whta',\n",
              " 'are',\n",
              " 'features',\n",
              " 'of',\n",
              " 'CyberSecurity',\n",
              " '?',\n",
              " 'What',\n",
              " 'are',\n",
              " 'the',\n",
              " 'Benefits',\n",
              " 'of',\n",
              " 'cybersecurity',\n",
              " '?',\n",
              " 'uses',\n",
              " 'of',\n",
              " 'cyber',\n",
              " 'security',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGEmsfDZdUp0",
        "outputId": "34a67f0e-13a3-4f22-a46b-67f273e723b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['Hi'], 'greeting'),\n",
              " (['How', 'are', 'you'], 'greeting'),\n",
              " (['Is', 'anyone', 'there', '?'], 'greeting'),\n",
              " (['Hello'], 'greeting'),\n",
              " (['Good', 'day'], 'greeting'),\n",
              " (['Bye'], 'goodbye'),\n",
              " (['See', 'you', 'later'], 'goodbye'),\n",
              " (['Goodbye'], 'goodbye'),\n",
              " (['Thanks'], 'thanks'),\n",
              " (['Thank', 'you'], 'thanks'),\n",
              " (['That', \"'s\", 'helpful'], 'thanks'),\n",
              " (['What', 'hours', 'are', 'you', 'open', '?'], 'hours'),\n",
              " (['What', 'are', 'your', 'hours', '?'], 'hours'),\n",
              " (['When', 'are', 'you', 'open', '?'], 'hours'),\n",
              " (['When', 'is', 'the', 'time', 'to', 'contact', '?'], 'hours'),\n",
              " (['At', 'what', 'time', 'do', 'you', 'provide', 'services', '?'], 'hours'),\n",
              " (['What', 'is', 'your', 'location', '?'], 'location'),\n",
              " (['Where', 'are', 'you', 'located', '?'], 'location'),\n",
              " (['What', 'is', 'your', 'address', '?'], 'location'),\n",
              " (['Where', 'is', 'your', 'company', 'situated', '?'], 'location'),\n",
              " (['How', 'can', 'we', 'contact', 'you', '?'], 'location'),\n",
              " (['How', 'can', 'I', 'contact', 'you', '?'], 'location'),\n",
              " (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'),\n",
              " (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'),\n",
              " (['Are', 'you', 'cash', 'only', '?'], 'payments'),\n",
              " (['Which', 'credit', 'cards', 'do', 'you', 'accept', '?'], 'payments'),\n",
              " (['What', 'are', 'the', 'services', 'that', 'you', 'provide', '?'],\n",
              "  'services'),\n",
              " (['Which', 'services', 'do', 'you', 'provide', '?'], 'services'),\n",
              " (['How', 'can', 'you', 'help', 'us', '?'], 'services'),\n",
              " (['What', 'can', 'you', 'do', '?'], 'services'),\n",
              " (['Do', 'you', 'provide', 'industrial', 'training', '?'], 'training'),\n",
              " (['Do', 'you', 'deliver', 'webinars', 'on', 'cybersecurity', '?'],\n",
              "  'training'),\n",
              " (['Do', 'you', 'provide', 'Summer', 'training', '?'], 'training'),\n",
              " (['Do', 'you', 'conduct', 'workshops', '?'], 'training'),\n",
              " (['What', 'is', 'mean', 'by', 'cryptography', '?'], 'Cryptography'),\n",
              " (['What', 'is', 'Cryptography', '?'], 'Cryptography'),\n",
              " (['What', 'is', 'a', 'Firewall', 'and', 'why', 'is', 'it', 'used', '?'],\n",
              "  'Firewall'),\n",
              " (['what', 'is', 'firewall', '?'], 'Firewall'),\n",
              " (['what', 'firewall', 'does', '?'], 'Firewall'),\n",
              " (['What', 'is', 'Penetration', 'Testing', '?'], 'Penetration Testing'),\n",
              " (['explain', 'Penetration', 'Testing', '?'], 'Penetration Testing'),\n",
              " (['What', 'is', 'in', 'cyber', 'security', '?'], 'Penetration Testing'),\n",
              " (['What', 'is', 'Vulnerability', 'Assessment', '?'],\n",
              "  'Vulnerability Assessment'),\n",
              " (['explain', 'Vulnerability', 'Assessment', '?'], 'Vulnerability Assessment'),\n",
              " (['What', 'is', 'VA', 'in', 'cyber', 'security', '?'],\n",
              "  'Vulnerability Assessment'),\n",
              " (['What', 'is', 'traceroute', '?'], 'traceroute'),\n",
              " (['explain', 'traceroute', '?'], 'traceroute'),\n",
              " (['What', 'is', 'a', 'VPN', '?'], 'VPN'),\n",
              " (['explain', 'vpn', '?'], 'VPN'),\n",
              " (['why', 'vpn', 'is', 'used', '?'], 'VPN'),\n",
              " (['what', 'are', 'black', 'hat', 'hackers', '?'], 'black hat hackers'),\n",
              " (['black', 'hat', 'hackers', '?'], 'black hat hackers'),\n",
              " (['what', 'are', 'White', 'hat', 'hackers', '?'], 'White hat hackers'),\n",
              " (['ethical', 'hackers', '?'], 'White hat hackers'),\n",
              " (['what', 'are', 'Grey', 'hat', 'hackers', '?'], 'Grey hat hackers'),\n",
              " (['Grey', 'hat', 'hackers', '?'], 'Grey hat hackers'),\n",
              " (['What', 'is', 'an', 'ARP', 'and', 'how', 'does', 'it', 'work', '?'], 'ARP'),\n",
              " (['what', 'is', 'ARP', '?'], 'ARP'),\n",
              " (['arp', 'in', 'cyber', 'security', '?'], 'ARP'),\n",
              " (['What', 'is', 'a', 'Botnet', '?'], 'Botnet'),\n",
              " (['Bonet', '?'], 'Botnet'),\n",
              " (['ssl', '?'], 'Secure Sockets Layers'),\n",
              " (['explain', 'ssl', '?'], 'Secure Sockets Layers'),\n",
              " (['What', 'is', 'ssl', '?'], 'Secure Sockets Layers'),\n",
              " (['tls', '?'], 'Transport Layer Security'),\n",
              " (['explain', 'tls', '?'], 'Transport Layer Security'),\n",
              " (['What', 'is', 'tls', '?'], 'Transport Layer Security'),\n",
              " (['What', 'is', 'Cognitive', 'Cybersecurity', '?'],\n",
              "  'Cognitive Cybersecurity'),\n",
              " (['explain', 'Cognitive', 'Cybersecurity', '?'], 'Cognitive Cybersecurity'),\n",
              " (['Cognitive', 'Cybersecurity', '?'], 'Cognitive Cybersecurity'),\n",
              " (['How', 'is', 'Encryption', 'different', 'from', 'Hashing', '?'],\n",
              "  'Encryption different from Hashing'),\n",
              " (['Encryption', 'different', 'from', 'Hashing', '?'],\n",
              "  'Encryption different from Hashing'),\n",
              " (['What', 'is', 'encryption', '?'], 'encryption'),\n",
              " (['encryption', '?'], 'encryption'),\n",
              " (['explain', 'encryption', '?'], 'encryption'),\n",
              " (['What', 'is', 'Decryption', '?'], 'Decryption'),\n",
              " (['Decryption', '?'], 'Decryption'),\n",
              " (['explain', 'Decryption', '?'], 'Decryption'),\n",
              " (['What', 'is', 'Hashing', '?'], 'Hashing'),\n",
              " (['Hashing', '?'], 'Hashing'),\n",
              " (['explain', 'Hashing', '?'], 'Hashing'),\n",
              " (['what', 'is', 'cybersecurity', '?'], 'cybersecurity'),\n",
              " (['cybersecurity', '?'], 'cybersecurity'),\n",
              " (['What', 'is', 'forward', 'secrecy', '?'], 'Forward Secrecy'),\n",
              " (['Forward', 'Secrecy', '?'], 'Forward Secrecy'),\n",
              " (['what', 'is', 'boot', 'sector', 'virus', '?'], 'boot sector virus'),\n",
              " (['how', 'boot', 'sector', 'virus', 'effects', '?'], 'boot sector virus'),\n",
              " (['what', 'is', 'direct', 'action', 'virus', '?'], 'direct action virus'),\n",
              " (['how', 'direct', 'action', 'virus', 'effects', '?'], 'direct action virus'),\n",
              " (['what', 'is', 'resident', 'virus', '?'], 'resident virus'),\n",
              " (['how', 'resident', 'virus', 'effects', '?'], 'resident virus'),\n",
              " (['what', 'is', 'multipartite', 'virus', '?'], 'multipartite virus'),\n",
              " (['how', 'multipartite', 'virus', 'effects', '?'], 'multipartite virus'),\n",
              " (['what', 'is', 'spacefiller', 'virus', '?'], 'spacefiller virus'),\n",
              " (['how', 'spacefiller', 'virus', 'effects', '?'], 'spacefiller virus'),\n",
              " (['Do', 'you', 'provide', 'home', 'delivery', '?'], ' file infector virus'),\n",
              " (['Do', 'you', 'deliver', 'the', 'food', '?'], ' file infector virus'),\n",
              " (['What', 'are', 'the', 'home', 'delivery', 'options', '?'],\n",
              "  ' file infector virus'),\n",
              " (['what', 'is', 'computer', 'virus', '?'], 'computer virus'),\n",
              " (['What', 'do', 'you', 'mean', 'by', 'computer', 'virus', '?'],\n",
              "  'computer virus'),\n",
              " (['Define', 'Computer', 'Virus'], 'computer virus'),\n",
              " (['Important', 'Features', 'of', 'a', 'Cybersecurity', '?'],\n",
              "  'Features of a Cybersecurity'),\n",
              " (['Why', 'we', 'need', 'CyberSecurity', '?'], 'Features of a Cybersecurity'),\n",
              " (['Whta', 'are', 'features', 'of', 'CyberSecurity', '?'],\n",
              "  'Features of a Cybersecurity'),\n",
              " (['What', 'are', 'the', 'Benefits', 'of', 'cybersecurity', '?'],\n",
              "  'Benefits of cybersecurity'),\n",
              " (['uses', 'of', 'cyber', 'security', '?'], 'Benefits of cybersecurity')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DIv1H7HdXo2",
        "outputId": "4fe0c537-ac52-4e9c-9d76-4d79e7f43dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greeting',\n",
              " 'goodbye',\n",
              " 'thanks',\n",
              " 'hours',\n",
              " 'location',\n",
              " 'payments',\n",
              " 'services',\n",
              " 'training',\n",
              " 'Cryptography',\n",
              " 'Firewall',\n",
              " 'Penetration Testing',\n",
              " 'Vulnerability Assessment',\n",
              " 'traceroute',\n",
              " 'VPN',\n",
              " 'black hat hackers',\n",
              " 'White hat hackers',\n",
              " 'Grey hat hackers',\n",
              " 'ARP',\n",
              " 'Botnet',\n",
              " 'Secure Sockets Layers',\n",
              " 'Transport Layer Security',\n",
              " 'Cognitive Cybersecurity',\n",
              " 'Encryption different from Hashing',\n",
              " 'encryption',\n",
              " 'Decryption',\n",
              " 'Hashing',\n",
              " 'cybersecurity',\n",
              " 'Forward Secrecy',\n",
              " 'boot sector virus',\n",
              " 'direct action virus',\n",
              " 'resident virus',\n",
              " 'multipartite virus',\n",
              " 'spacefiller virus',\n",
              " ' file infector virus',\n",
              " 'computer virus',\n",
              " 'Features of a Cybersecurity',\n",
              " 'Benefits of cybersecurity']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing Stemming by using stemmer.stem() nd lower each word \n",
        "#Running loop in words[] and ignoring punctuation marks present in ignore[]\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
        "words = sorted(list(set(words)))  #Removing Duplicates in words[]\n",
        "\n",
        "#Removing Duplicate Classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "#Printing length of lists we formed\n",
        "print(len(documents),\"Documents \\n\")\n",
        "print(len(classes),\"Classes \\n\")\n",
        "print(len(words), \"Stemmed Words \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B69nVecdZ_q",
        "outputId": "656abdac-e7e9-4f82-f3ea-154103f0c706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106 Documents \n",
            "\n",
            "37 Classes \n",
            "\n",
            "118 Stemmed Words \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[:3]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODRnGMLUdb-t",
        "outputId": "5326d829-5bfb-4ae7-e8cb-0b052980d225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['Hi'], 'greeting'),\n",
              " (['How', 'are', 'you'], 'greeting'),\n",
              " (['Is', 'anyone', 'there', '?'], 'greeting')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words[:3]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNZKAFyFdeDB",
        "outputId": "24c5f2e4-b221-493d-8e07-f5d00d02f936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'s\", 'a', 'acceiv']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Training Data which will be furthur used for training\n",
        "training = [] ######## X\n",
        "output = []  ######### Y\n",
        "\n",
        "#Creating empty array for output\n",
        "output_empty = [0] * len(classes) ######## initialization with 0\n",
        "\n",
        "#Creating Training set and bag of words for each sentence\n",
        "for doc in documents:\n",
        "    bag = [] #Initialising empty bag of words\n",
        "\n",
        "    pattern_words = doc[0] #Storing list of tokenized words for the documents[] tp pattern_words\n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Again Stemming each word from pattern_words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Creating bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        \n",
        "    #It will give output 1 for curent tag and 0 for all other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] =1\n",
        "    training.append([bag, output_row])"
      ],
      "metadata": {
        "id": "isbLM27_dfxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training) #Suffling the data or features\n",
        "training = np.array(training) #Converting training data into numpy array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_EiUQWxdm9b",
        "outputId": "46a4e528-2c3d-4ae8-969a-2bf0ae8acde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training) #Suffling the data or features\n",
        "training = np.array(training) #Converting training data into numpy array"
      ],
      "metadata": {
        "id": "F1Qu3ul-dprb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Training Lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "ozpYOVrFdriV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() #Reset Underlying Graph data\n",
        "\n",
        "#Building our own Neural Network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "#Defining Model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\") \n",
        "\n",
        "#Now we have setup model, now we need to train that model by fitting data into it by model.fit()\n",
        "#n_epoch is the number of times that model will se our data during training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True) \n",
        "model.save(\"model.tflearn\") #Saving the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zmrafkqdtZl",
        "outputId": "4a04aa3d-bd7b-4d45-8233-c968de28d680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 13999  | total loss: \u001b[1m\u001b[32m0.99147\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 1000 | loss: 0.99147 - acc: 0.9522 -- iter: 104/106\n",
            "Training Step: 14000  | total loss: \u001b[1m\u001b[32m0.89263\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 1000 | loss: 0.89263 - acc: 0.9570 -- iter: 106/106\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing pickle module\n",
        "import pickle\n",
        "\n",
        "#Dumping training data by using dump() and writing it into training_data in binary mode\n",
        "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
      ],
      "metadata": {
        "id": "K5s77ClvdwEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Restoring all data structure\n",
        "data = pickle.load(open(\"training_data\",\"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "metadata": {
        "id": "wOTM8Aq4d2l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"intents.json\") as json_data:\n",
        "    intents = json.load(json_data)  #Loading our json_data"
      ],
      "metadata": {
        "id": "cXVKoPNid7VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the saved model\n",
        "model.load(\"./model.tflearn\") #Loading training model which we saved\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvGuH5qZd961",
        "outputId": "70617c06-f5b9-4495-c32a-351389c10126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning User Input\n",
        "def clean_up_sentence(sentence):\n",
        "    \n",
        "    # Tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence) #Again tokenizing the sentence\n",
        "    \n",
        "    #Stemming each word from the user's input\n",
        "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "\n",
        "    return sentence_words\n",
        "\n",
        "#Returning bag of words array: 0 or 1 or each word in the bag that exists in as we have declared in above lines\n",
        "def bow(sentence, words, show_details=False):\n",
        "    \n",
        "    #Tokenizing the user input\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    \n",
        "    #Generating bag of words from the sentence that user entered\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\"% w)\n",
        "    return(np.array(bag))"
      ],
      "metadata": {
        "id": "FO9MbwH4eBdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding some context to the conversation for better results.\n",
        "\n",
        "context = {} #Create a dictionary to hold user's context\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "\n",
        "\n",
        "def classify(sentence):\n",
        "    \n",
        "    #Generating probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    \n",
        "    #Filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    \n",
        "    #Sorting by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    \n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    \n",
        "    #If we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        \n",
        "        #Loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                \n",
        "                #Find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    \n",
        "                    #Set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        \n",
        "                        #A random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "metadata": {
        "id": "mC_A8RZXeVpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response('how are you')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35Hu9AIeYo4",
        "outputId": "4d3da669-5585-415d-82fe-798e78984e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to see you again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## Success Case #########\n",
        "\n",
        "response('What services do you give?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K84x5LLPecrD",
        "outputId": "abea5ca9-cbd0-4703-8f35-b2354613b139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We provide Web Penetration Testing,Android Penetration Testing,Docker Penetration Testing,Vulnerability Assessment,Cyber Crime investigation and many more services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Failure Case #########\n",
        "\n",
        "response('What is your payment method?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPbpWyF6ee1P",
        "outputId": "16421505-f71f-492a-ca79-e1d36bdb6b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decryption is the process of taking encoded or encrypted text or other data and converting it back into text that you or the computer can read and understand. This term could be used to describe a method of unencrypting the data manually or unencrypting the data using the proper codes or keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVVrrGG0g6IE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}